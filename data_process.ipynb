{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机抽取500条数据保存到新文件，数据不足则报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_file = r\"F:\\LLM\\lora_finetune_dataset_functioncall.jsonl\"\n",
    "output_file = r\"F:\\LLM\\lora_finetune_dataset_functioncall_sample600.jsonl\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "num = 500\n",
    "if len(lines) < num:\n",
    "    raise ValueError(\"数据不足600条，当前仅有{}条数据。\".format(len(lines)))\n",
    "\n",
    "sample_lines = random.sample(lines, num)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in sample_lines:\n",
    "        f.write(line)\n",
    "print(\"成功抽取600条数据并保存到：\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建Hugging Face数据仓库并上传文件\n",
    "\n",
    "此代码创建一个Hugging Face数据仓库并将指定的JSONL文件上传到该仓库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi, create_repo, upload_file\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "repo_id = \"zyss1/ljz_chains_datasets\"  \n",
    "\n",
    "try:\n",
    "    create_repo(repo_id, token=HF_TOKEN, repo_type=\"dataset\", exist_ok=True)\n",
    "    print(f\"数据仓库 {repo_id} 创建或已存在。\")\n",
    "except Exception as e:\n",
    "    print(f\"创建仓库时出错：{e}\")\n",
    "\n",
    "file_path = r\"F:\\LLM\\lora_finetune_dataset_functioncall_sample600.jsonl\"\n",
    "path_in_repo = \"lora_finetune_dataset_functioncall_sample600.jsonl\"\n",
    "\n",
    "try:\n",
    "    upload_file(\n",
    "        path_or_fileobj=file_path,\n",
    "        path_in_repo=path_in_repo,\n",
    "        repo_id=repo_id,\n",
    "        token=HF_TOKEN,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    print(f\"文件已成功上传到 {repo_id} 数据仓库。\")\n",
    "except Exception as e:\n",
    "    print(f\"上传文件时出错：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集处理与预处理\n",
    "\n",
    "该代码加载Hugging Face上的数据集并对其进行预处理。它修复了文本中的JSON格式错误，处理了`<tool_call>`、`<tool_response>`和`<think>`标签。接着，它使用指定的模型和tokenizer进行消息的格式化，并将数据集划分为训练集和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "dataset_name = \"zyss1/ljz_chains_datasets\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    ")\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.rename_column(\"conversation\", \"messages\")\n",
    "\n",
    "print(\"Dataset sample structure:\")\n",
    "sample = dataset[\"train\"][0]\n",
    "print(type(sample[\"messages\"]))\n",
    "print(sample[\"messages\"] if isinstance(sample[\"messages\"], list) else \"Not a list\")\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "def fix_escaped_quotes(text):\n",
    "    \"\"\"修复文本中过度转义的引号\"\"\"\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    text = text.replace('\\\\\"', '\"')\n",
    "    return text\n",
    "\n",
    "def fix_json_in_tags(text):\n",
    "    \"\"\"修复标签内部的JSON格式\"\"\"\n",
    "    \n",
    "    def process_tag_content(match):\n",
    "        content = match.group(2)\n",
    "        fixed_content = fix_escaped_quotes(content)\n",
    "        try:\n",
    "            json_obj = json.loads(fixed_content)\n",
    "            formatted_json = json.dumps(json_obj, ensure_ascii=False)\n",
    "            return match.group(1) + formatted_json + match.group(3)\n",
    "        except:\n",
    "            return match.group(1) + fixed_content + match.group(3)\n",
    "    \n",
    "    text = re.sub(r'(<tool_call>)(.*?)(</tool_call>)', process_tag_content, text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'(<tool_response>)(.*?)(</tool_response>)', process_tag_content, text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'(<think>)(.*?)(</think>)', process_tag_content, text, flags=re.DOTALL)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    \n",
    "    if not isinstance(messages, list):\n",
    "        return {\"text\": \"Error: unexpected message format\"}\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        if isinstance(msg, dict) and \"content\" in msg and isinstance(msg[\"content\"], str):\n",
    "            if msg.get(\"role\") == \"model\":\n",
    "                try:\n",
    "                    content_dict = json.loads(msg[\"content\"])\n",
    "                    \n",
    "                    if \"steps\" in content_dict and isinstance(content_dict[\"steps\"], list):\n",
    "                        for step in content_dict[\"steps\"]:\n",
    "                            if \"think\" in step:\n",
    "                                step[\"think\"] = fix_json_in_tags(step[\"think\"])\n",
    "                            if \"tool_call\" in step:\n",
    "                                step[\"tool_call\"] = fix_json_in_tags(step[\"tool_call\"])\n",
    "                            if \"tool_response\" in step:\n",
    "                                step[\"tool_response\"] = fix_json_in_tags(step[\"tool_response\"])\n",
    "             \n",
    "                    messages[i][\"content\"] = json.dumps(content_dict, ensure_ascii=False)\n",
    "                except json.JSONDecodeError:\n",
    "                    messages[i][\"content\"] = fix_json_in_tags(msg[\"content\"])\n",
    "            else:\n",
    "                messages[i][\"content\"] = fix_json_in_tags(msg[\"content\"])\n",
    "\n",
    "    if messages and isinstance(messages[0], dict) and \"role\" in messages[0]:\n",
    "        if messages[0][\"role\"] == \"system\":\n",
    "            system_message_content = messages[0][\"content\"]\n",
    "            if len(messages) > 1:\n",
    "                messages[1][\"content\"] = (\n",
    "                    system_message_content +\n",
    "                    \"Also, before making a call to a function take the time to plan the function to take. \"\n",
    "                    \"Make that thinking process between <think>{your thoughts}</think>\\n\\n\" +\n",
    "                    messages[1][\"content\"]\n",
    "                )\n",
    "            messages = messages[1:]\n",
    "    \n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "dataset = dataset.map(preprocess, remove_columns=[\"messages\"])\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][85][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(original_tokenizer.tokenize(\"<start_of_turn>\"))\n",
    "print(original_tokenizer.tokenize(\"<end_of_turn>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特殊标记处理与验证\n",
    "\n",
    "此代码通过定义自定义的特殊标记，初始化tokenizer，并验证这些标记是否成功添加到tokenizer中。它通过检查标记是否被正确分解、词汇表大小的变化以及对含有多个特殊标记的句子的编码与解码，确保特殊标记的正确性和一致性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call = \"<tool_call>\"\n",
    "    eotool_call = \"</tool_call>\"\n",
    "    tool_response = \"<tool_response>\"\n",
    "    eotool_response = \"</tool_response>\"\n",
    "    answer = \"<answer>\"\n",
    "    eoanswer = \"</answer>\"\n",
    "    start_of_turn = \"<start_of_turn>\"\n",
    "    end_of_turn = \"<end_of_turn>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    \n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list()\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    ")\n",
    "\n",
    "print(\"特殊标记检查:\")\n",
    "for token in ChatmlSpecialTokens.list():\n",
    "    in_special = token in tokenizer.all_special_tokens\n",
    "    print(f\"{token}: {'在特殊标记列表中' if in_special else '不在特殊标记列表中'}\")\n",
    "\n",
    "print(\"\\n分词测试:\")\n",
    "for token in ChatmlSpecialTokens.list():\n",
    "    tokens = tokenizer.tokenize(token)\n",
    "    token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "    print(f\"{token} -> {tokens} (token_ids: {token_ids})\")\n",
    "    \n",
    "    if len(tokens) == 1:\n",
    "        print(f\"✓ 正确: 被识别为单个token\")\n",
    "    else:\n",
    "        print(f\"✗ 错误: 被分解为多个token\")\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(f\"\\n原始词汇表大小: {len(original_tokenizer)}\")\n",
    "print(f\"修改后词汇表大小: {len(tokenizer)}\")\n",
    "print(f\"增加的token数量: {len(tokenizer) - len(original_tokenizer)}\")\n",
    "\n",
    "test_text = \"<start_of_turn>human\\n你好<end_of_turn><eos>\\n<start_of_turn>model\\n<think>用户打招呼，我应该回应</think><tool_call>{'name': 'test'}</tool_call><answer>你好！有什么我可以帮助你的吗？</answer><end_of_turn><eos>\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"\\n测试文本编码再解码:\")\n",
    "print(f\"原始文本: {test_text}\")\n",
    "print(f\"解码后文本: {decoded}\")\n",
    "print(f\"文本是否保持一致: {'是' if test_text == decoded else '否'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型并调整token嵌入\n",
    "\n",
    "该代码加载预训练模型并调整其token嵌入大小以匹配新的tokenizer词汇表。接着，模型被转换为bfloat16格式，以提高计算性能并减少内存占用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation='eager',\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(torch.bfloat16)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
